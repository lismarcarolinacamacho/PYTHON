{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scrapping_wikipdia.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOmEKfuksVMwbWiWYKn2q8a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lismarcarolinacamacho/PYTHON/blob/main/scrapping_wikipdia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lista de palabras mas buscadas en un momento dado"
      ],
      "metadata": {
        "id": "v-GM2A559rm3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NrPaV0n4c_pB",
        "outputId": "6e349d55-8cab-4f13-c06a-5d4def8960c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://en.wikipedia.org/w/api.php?format=json&action=query&list=search&srsearch=-f\n",
            "url: https://en.wikipedia.org/wikiNCIS (TV series)\n",
            "| Palabra      |   Frecuencia |   Porcentaje de frecuencia |\n",
            "|--------------+--------------+----------------------------|\n",
            "| garfield     |            4 |                     1.3333 |\n",
            "| united       |            4 |                     1.3333 |\n",
            "| states       |            4 |                     1.3333 |\n",
            "| recorded     |            4 |                     1.3333 |\n",
            "| painting     |            4 |                     1.3333 |\n",
            "| also         |            3 |                     1      |\n",
            "| day          |            3 |                     1      |\n",
            "| sibelius     |            3 |                     1      |\n",
            "| written      |            3 |                     1      |\n",
            "| many         |            3 |                     1      |\n",
            "| ehrling      |            3 |                     1      |\n",
            "| milky        |            3 |                     1      |\n",
            "| way          |            3 |                     1      |\n",
            "| th           |            2 |                     0.6667 |\n",
            "| president    |            2 |                     0.6667 |\n",
            "| september    |            2 |                     0.6667 |\n",
            "| general      |            2 |                     0.6667 |\n",
            "| civil        |            2 |                     0.6667 |\n",
            "| election     |            2 |                     0.6667 |\n",
            "| presidential |            2 |                     0.6667 |\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import operator \n",
        "\n",
        "import json\n",
        "from tabulate import tabulate\n",
        "import sys\n",
        "from stop_words import get_stop_words\n",
        "\n",
        "def getWordList(url):\n",
        "  word_list = []\n",
        "\n",
        "  #raw data\n",
        "  source_code = requests.get(url)\n",
        "  #convert to text\n",
        "  plain_text = source_code.text\n",
        "  #lxml format\n",
        "  soup = BeautifulSoup(plain_text, 'lxml')\n",
        "\n",
        "  #find the words in the paragraph tag\n",
        "  for text in soup.findAll('p'):\n",
        "    if text.text is None:\n",
        "      continue\n",
        "    #content\n",
        "    content = text.text\n",
        "    #lowercase and split into array\n",
        "    words = content.lower().split()\n",
        "\n",
        "    #for each word\n",
        "    for word in words:\n",
        "      #remove non-chars\n",
        "      cleaned_word = clean_word(word)\n",
        "      #if the is still something here\n",
        "      if len(cleaned_word)> 0:\n",
        "        #add it to our words list\n",
        "        word_list.append(cleaned_word)\n",
        "\n",
        "  return word_list\n",
        "\n",
        "#clean word with regex\n",
        "def clean_word(word):\n",
        "  cleaned_word = re.sub('[^A-Za-z]+', '', word)\n",
        "  return cleaned_word\n",
        "\n",
        "def createFrecuencyTable(word_list):\n",
        "  #wourd count\n",
        "  word_count = {}\n",
        "  for word in word_list:\n",
        "    #index is the word\n",
        "    if word in word_count:\n",
        "      word_count[word] += 1\n",
        "    else:\n",
        "      word_count[word] = 1\n",
        "  return word_count\n",
        "\n",
        "#remove stop words\n",
        "def remove_stop_words(frecuency_list):\n",
        "  stop_words = get_stop_words('en')\n",
        "\n",
        "  temp_list = []\n",
        "  for key, value in frecuency_list:\n",
        "    if key not in stop_words:\n",
        "      temp_list.append([key, value])\n",
        "  return temp_list\n",
        "\n",
        "#access wiki API, JSON format, query it for  data . search type . show list of posibilities\n",
        "\n",
        "wikipedia_API_link = 'https://en.wikipedia.org/w/api.php?format=json&action=query&list=search&srsearch='\n",
        "wikipedia_link = 'https://en.wikipedia.org/wiki'\n",
        "\n",
        "# if the search word is too small, throw error\n",
        "\n",
        "if (len(sys.argv)<2):\n",
        "  print(\"Enter valid string: \")\n",
        "  exit()\n",
        "\n",
        "#get the search word\n",
        "\n",
        "string_query = sys.argv[1]\n",
        "#to remove stop wordss or not\n",
        "if (len(sys.argv)>2):\n",
        "  search_mode = True\n",
        "else:\n",
        "  search_mode = False\n",
        "\n",
        "#create our url\n",
        "\n",
        "url = wikipedia_API_link + string_query\n",
        "print(url)\n",
        "\n",
        "#try except block. simple way to deal with exceptions \n",
        "# great for HTTPS request\n",
        "\n",
        "try:\n",
        "  #use requests to retrieve row data from wiki API URL we just consrtucted\n",
        "\n",
        "  response = requests.get(url)\n",
        "  #format that data as Json dictionary\n",
        "  data = json.loads(response.content.decode(\"utf-8\")) \n",
        "\n",
        "  #page title, first option \n",
        "  # show this in web browser\n",
        "\n",
        "  wikipedia_page_tag = data['query']['search'][0]['title']\n",
        "\n",
        "  #get actual wiki page based in retrieved title \n",
        "  url = wikipedia_link + wikipedia_page_tag\n",
        "  print(\"url: \"+url)\n",
        "  #get list of words from that page \n",
        "  page_word_list = getWordList(url)\n",
        "  #get list of words counts, dictionary \n",
        "  page_word_count = createFrecuencyTable(page_word_list)\n",
        "  #short the table by frecuency count\n",
        "  sorted_word_frecuency_list = sorted(page_word_count.items(),\n",
        "                                 key=operator.itemgetter(1), reverse= True)\n",
        "  #remove stop words if the user specified\n",
        "  if(search_mode):\n",
        "    sorted_word_frecuency_list = remove_stop_words(sorted_word_frecuency_list)\n",
        "\n",
        "  #sum the total words to calculate frecuencies\n",
        "  total_words_sum = 0\n",
        "  for key, value in sorted_word_frecuency_list:\n",
        "    total_words_sum = total_words_sum + value\n",
        "\n",
        "  #just get the top 20 words \n",
        "\n",
        "  if len(sorted_word_frecuency_list) > 20:\n",
        "    sorted_word_frecuency_list = sorted_word_frecuency_list[:20]\n",
        "\n",
        "  #create our final list which contain list, frecuency (word count, percentage)\n",
        "  final_list = []\n",
        "  for key, value in sorted_word_frecuency_list:\n",
        "    percentage_value = float(value*100) / total_words_sum\n",
        "    final_list.append([key, value, round(percentage_value, 4)])\n",
        "  \n",
        "  #header before the table \n",
        "  print_headers = ['Palabra', 'Frecuencia', 'Porcentaje de frecuencia' ]\n",
        "\n",
        "  # print the table with tabulate \n",
        "  print (tabulate(final_list, headers = print_headers, tablefmt= 'orgtbl' ))\n",
        "\n",
        "#throw an exception in case it breaks\n",
        "\n",
        "except requests.exceptions.Timeout:\n",
        "  print(\"The server didn't respond. Please try again later\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install stop_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4zSbua_jZBr",
        "outputId": "9ac5d569-0546-4dc7-959d-f50937a19168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stop_words\n",
            "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-py3-none-any.whl size=32911 sha256=0c4f50d77b428aa7f466f0cb4ab9a6e448fd8ba38edb664eb411d87d20acfb51\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/86/b2/277b10b1ce9f73ce15059bf6975d4547cc4ec3feeb651978e9\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IjLO2NESjZFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UAgxt-f_jZIu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}